# muGrid Multi-Stage Build Recipe
#
# This recipe uses multi-stage builds to create a minimal runtime container.
# The build stage contains all compilers and development tools; the runtime
# stage contains only the necessary shared libraries and Python packages.
#
# Build-time configuration via --build-arg:
#   MUGRID_ARCH      - Target CPU architecture (default: native)
#                      Examples: znver4, skylake, sandybridge, native
#   MUGRID_GPU       - GPU backend: none, cuda, rocm (default: none)
#   MUGRID_VERSION   - muGrid version/branch (default: 0.102.0)
#
# Example builds:
#   # CPU-only for Zen4:
#   apptainer build --build-arg MUGRID_ARCH=znver4 --build-arg MUGRID_GPU=none \
#       mugrid.sif mugrid-multistage.def
#
#   # CUDA for Zen4 (requires CUDA base image):
#   apptainer build --build-arg MUGRID_ARCH=znver4 --build-arg MUGRID_GPU=cuda \
#       mugrid.sif mugrid-multistage.def
#
#   # ROCm for Zen4 (requires ROCm base image):
#   apptainer build --build-arg MUGRID_ARCH=znver4 --build-arg MUGRID_GPU=rocm \
#       mugrid.sif mugrid-multistage.def
#
# Note: The base image (netcdf_fftw_pfft.sif) must match the GPU backend.
#       - For CUDA: symlink to netcdf_fftw_pfft-cuda.sif
#       - For ROCm: symlink to netcdf_fftw_pfft-rocm.sif
#       - For CPU:  symlink to netcdf_fftw_pfft.sif (CPU-only stack)
#
# Dependencies: muGrid requires PnetCDF (parallel NetCDF) for I/O.

######################################################################
# STAGE 1: BUILD
# Full development environment with compilers, headers, and build tools
######################################################################

Bootstrap: localimage
From: netcdf_fftw_pfft.sif
Stage: build

%arguments
MUGRID_ARCH=native
MUGRID_GPU=none
MUGRID_VERSION=0.102.0

%post
    # ================================================================
    # Configuration from build arguments
    # ================================================================
    MUGRID_ARCH="{{ MUGRID_ARCH }}"
    MUGRID_GPU="{{ MUGRID_GPU }}"
    MUGRID_VERSION="{{ MUGRID_VERSION }}"

    echo "=========================================="
    echo "muGrid Build Configuration:"
    echo "  Architecture: ${MUGRID_ARCH}"
    echo "  GPU Backend:  ${MUGRID_GPU}"
    echo "  Version:      ${MUGRID_VERSION}"
    echo "=========================================="

    # ================================================================
    # Environment setup
    # ================================================================
    if [ -e /usr/share/modulefiles ]; then
        export MODULEPATH=/usr/share/modulefiles
        . /usr/share/lmod/lmod/init/sh
        module load mpi
    fi

    export PATH=/usr/local/bin:$PATH
    export LD_RUN_PATH=/usr/local/lib:$LD_LIBRARY_PATH
    export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH
    export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH

    export DEBIAN_FRONTEND=noninteractive
    export DEBCONF_NONINTERACTIVE_SEEN=true

    # ================================================================
    # Install build dependencies
    # ================================================================
    if command -v dnf > /dev/null 2>&1; then
        dnf install -y boost-devel
    else
        apt-get update
        apt-get install -y --no-install-recommends libboost-test-dev
    fi

    # Python build dependencies
    pip install --break-system-packages --no-cache-dir pybind11 numpy mpi4py NuMPI

    # ================================================================
    # Clone muGrid source
    # ================================================================
    export MUGRID_SRC_DIR=/tmp/mugrid
    export MUGRID_BUILD_DIR=/tmp/mugrid-build

    rm -rf ${MUGRID_SRC_DIR} ${MUGRID_BUILD_DIR}
    git clone --recurse-submodules -b ${MUGRID_VERSION} \
        https://github.com/muSpectre/muGrid.git ${MUGRID_SRC_DIR}

    # ================================================================
    # Configure optimization flags
    # ================================================================
    if [ "${MUGRID_ARCH}" = "native" ]; then
        OPT_FLAGS="-O3 -march=native"
    else
        OPT_FLAGS="-O3 -march=${MUGRID_ARCH}"
    fi

    # ================================================================
    # Configure GPU backend
    # ================================================================
    GPU_CMAKE_FLAGS=""

    case "${MUGRID_GPU}" in
        cuda)
            echo "Configuring CUDA backend"
            if [ -d "/usr/local/cuda" ]; then
                export CUDA_HOME=/usr/local/cuda
                export PATH=$CUDA_HOME/bin:$PATH
                export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
            fi
            GPU_CMAKE_FLAGS="-DMUGRID_ENABLE_CUDA=ON"
            # H200=sm_90 (Hopper), A100=sm_80 (Ampere)
            GPU_CMAKE_FLAGS="${GPU_CMAKE_FLAGS} -DCMAKE_CUDA_ARCHITECTURES=80;90"
            ;;
        rocm)
            echo "Configuring ROCm/HIP backend"
            if [ -d "/opt/rocm" ]; then
                export ROCM_PATH=/opt/rocm
                export PATH=$ROCM_PATH/bin:$PATH
                export LD_LIBRARY_PATH=$ROCM_PATH/lib:$LD_LIBRARY_PATH
            fi
            GPU_CMAKE_FLAGS="-DMUGRID_ENABLE_HIP=ON"
            # MI300A=gfx942, MI200=gfx90a
            GPU_CMAKE_FLAGS="${GPU_CMAKE_FLAGS} -DCMAKE_HIP_ARCHITECTURES=gfx90a;gfx942"
            ;;
        none|*)
            echo "Configuring CPU-only build"
            ;;
    esac

    # ================================================================
    # Build muGrid
    # ================================================================
    mkdir -p ${MUGRID_BUILD_DIR}
    cd ${MUGRID_BUILD_DIR}

    cmake ${MUGRID_SRC_DIR} \
        -DCMAKE_BUILD_TYPE=Release \
        -DCMAKE_C_FLAGS="${OPT_FLAGS}" \
        -DCMAKE_CXX_FLAGS="${OPT_FLAGS}" \
        -DCMAKE_INSTALL_PREFIX=/usr/local \
        -DCMAKE_PREFIX_PATH=/usr/local \
        -DMUGRID_ENABLE_MPI=ON \
        ${GPU_CMAKE_FLAGS}

    make -j$(nproc)
    make install

    # Clean up build directories
    rm -rf ${MUGRID_SRC_DIR} ${MUGRID_BUILD_DIR}

    # ================================================================
    # Prepare staging area with only runtime files
    # Copy everything needed for runtime - ensures ABI compatibility
    # ================================================================
    mkdir -p /staging/usr/local/lib
    mkdir -p /staging/usr/local/bin
    mkdir -p /staging/usr/local/share
    mkdir -p /staging/usr/lib
    mkdir -p /staging/etc

    # -------------------------------
    # Copy ALL shared libraries from /usr/local/lib
    # This is more robust than selectively copying due to complex HPC dependency chains
    # (MPI, UCX, UCC, libfabric providers, etc.)
    # -------------------------------
    # Copy all .so files (but not .a static libraries or .la libtool files)
    find /usr/local/lib -name "*.so*" -type f -exec cp -a {} /staging/usr/local/lib/ \;
    find /usr/local/lib -name "*.so*" -type l -exec cp -a {} /staging/usr/local/lib/ \;
    # Copy plugin directories (UCX, UCC, OpenMPI, PMIx)
    cp -a /usr/local/lib/ucx /staging/usr/local/lib/ 2>/dev/null || true
    cp -a /usr/local/lib/ucc /staging/usr/local/lib/ 2>/dev/null || true
    cp -a /usr/local/lib/openmpi /staging/usr/local/lib/ 2>/dev/null || true
    cp -a /usr/local/lib/pmix /staging/usr/local/lib/ 2>/dev/null || true

    # CUDA stubs (needed by mpi4py even for CPU-only builds, since mpi4py
    # was compiled with CUDA support in the base container)
    if [ -d "/usr/local/cuda/lib64/stubs" ]; then
        mkdir -p /staging/usr/local/cuda/lib64
        cp -a /usr/local/cuda/lib64/stubs /staging/usr/local/cuda/lib64/
        # Also copy minimal CUDA runtime libraries
        cp -a /usr/local/cuda/lib64/libcudart.so* /staging/usr/local/cuda/lib64/ 2>/dev/null || true
        cp -a /usr/local/cuda/lib64/libOpenCL.so* /staging/usr/local/cuda/lib64/ 2>/dev/null || true
    fi
    # MPI binaries
    cp -a /usr/local/bin/mpirun /staging/usr/local/bin/ 2>/dev/null || true
    cp -a /usr/local/bin/mpiexec /staging/usr/local/bin/ 2>/dev/null || true
    cp -a /usr/local/bin/orte* /staging/usr/local/bin/ 2>/dev/null || true
    cp -a /usr/local/bin/ompi* /staging/usr/local/bin/ 2>/dev/null || true
    # MPI data files
    cp -a /usr/local/share/openmpi /staging/usr/local/share/ 2>/dev/null || true
    cp -a /usr/local/share/pmix /staging/usr/local/share/ 2>/dev/null || true
    cp -a /usr/local/share/hwloc /staging/usr/local/share/ 2>/dev/null || true

    # -------------------------------
    # PnetCDF runtime (required for muGrid I/O)
    # -------------------------------
    cp -a /usr/local/lib/libpnetcdf*.so* /staging/usr/local/lib/ 2>/dev/null || true

    # -------------------------------
    # muGrid libraries and Python bindings
    # -------------------------------
    cp -a /usr/local/lib/libmugrid*.so* /staging/usr/local/lib/ 2>/dev/null || true
    cp -a /usr/local/lib/python3* /staging/usr/local/lib/
    # CMake config for downstream builds (optional but useful)
    mkdir -p /staging/usr/local/lib/cmake
    cp -a /usr/local/lib/cmake/muGrid /staging/usr/local/lib/cmake/ 2>/dev/null || true

    # -------------------------------
    # System runtime libraries (copy from build container for ABI match)
    # -------------------------------
    # Boost test runtime
    cp -a /usr/lib/x86_64-linux-gnu/libboost_unit_test_framework*.so* /staging/usr/lib/ 2>/dev/null || true
    # OpenMP runtime
    cp -a /usr/lib/x86_64-linux-gnu/libgomp*.so* /staging/usr/lib/ 2>/dev/null || true
    # InfiniBand/RDMA (for HPC networking)
    cp -a /usr/lib/x86_64-linux-gnu/libibverbs*.so* /staging/usr/lib/ 2>/dev/null || true
    cp -a /usr/lib/x86_64-linux-gnu/librdmacm*.so* /staging/usr/lib/ 2>/dev/null || true
    cp -a /usr/lib/x86_64-linux-gnu/libnl*.so* /staging/usr/lib/ 2>/dev/null || true
    # EFA (AWS Elastic Fabric Adapter) - libfabric provider
    cp -a /usr/lib/x86_64-linux-gnu/libefa*.so* /staging/usr/lib/ 2>/dev/null || true
    mkdir -p /staging/usr/lib/libibverbs
    cp -a /usr/lib/x86_64-linux-gnu/libibverbs/*.so* /staging/usr/lib/libibverbs/ 2>/dev/null || true
    # NUMA
    cp -a /usr/lib/x86_64-linux-gnu/libnuma*.so* /staging/usr/lib/ 2>/dev/null || true
    # Other dependencies
    cp -a /usr/lib/x86_64-linux-gnu/libltdl*.so* /staging/usr/lib/ 2>/dev/null || true
    cp -a /usr/lib/x86_64-linux-gnu/libxml2*.so* /staging/usr/lib/ 2>/dev/null || true

    # -------------------------------
    # GPU runtime libraries (only if GPU backend enabled)
    # -------------------------------
    case "${MUGRID_GPU}" in
        cuda)
            echo "Copying CUDA runtime libraries..."
            mkdir -p /staging/usr/local/cuda/lib64
            # Runtime libraries only (no static libs, no compiler)
            cp -a /usr/local/cuda/lib64/libcudart.so* /staging/usr/local/cuda/lib64/ 2>/dev/null || true
            cp -a /usr/local/cuda/lib64/libcufft.so* /staging/usr/local/cuda/lib64/ 2>/dev/null || true
            cp -a /usr/local/cuda/lib64/libcublas.so* /staging/usr/local/cuda/lib64/ 2>/dev/null || true
            cp -a /usr/local/cuda/lib64/libcublasLt.so* /staging/usr/local/cuda/lib64/ 2>/dev/null || true
            # nvidia-ml for GPU queries
            cp -a /usr/local/cuda/lib64/libnvrtc*.so* /staging/usr/local/cuda/lib64/ 2>/dev/null || true
            ;;
        rocm)
            echo "Copying ROCm runtime libraries..."
            mkdir -p /staging/opt/rocm/lib
            cp -a /opt/rocm/lib/libamdhip64.so* /staging/opt/rocm/lib/ 2>/dev/null || true
            cp -a /opt/rocm/lib/libhsa-runtime64.so* /staging/opt/rocm/lib/ 2>/dev/null || true
            cp -a /opt/rocm/lib/librocfft.so* /staging/opt/rocm/lib/ 2>/dev/null || true
            cp -a /opt/rocm/lib/libamd_comgr.so* /staging/opt/rocm/lib/ 2>/dev/null || true
            ;;
    esac

    # -------------------------------
    # Build info
    # -------------------------------
    cat > /staging/etc/mugrid-build-info << EOF
MUGRID_VERSION=${MUGRID_VERSION}
MUGRID_ARCH=${MUGRID_ARCH}
MUGRID_GPU=${MUGRID_GPU}
BUILD_DATE=$(date -Iseconds)
EOF

    # Report staging size
    echo "=========================================="
    echo "Staging area summary:"
    du -sh /staging/*
    du -sh /staging/usr/local/lib/* 2>/dev/null | head -15
    echo "=========================================="


######################################################################
# STAGE 2: RUNTIME
# Minimal Ubuntu container with only essential runtime components
# Must match Python version from build stage (Python 3.12)
######################################################################

Bootstrap: docker
From: ubuntu:24.04
Stage: runtime

%files from build
    /staging/usr/local/ /usr/local
    /staging/usr/lib/ /usr/lib
    /staging/etc/mugrid-build-info /etc/mugrid-build-info

%post
    export DEBIAN_FRONTEND=noninteractive
    MUGRID_GPU="{{ MUGRID_GPU }}"

    # ================================================================
    # Install only essential runtime packages
    # Most libraries are copied from build stage for ABI compatibility
    # ================================================================
    apt-get update
    apt-get install -y --no-install-recommends \
        python3 \
        python3-pip \
        python3-numpy \
        libstdc++6 \
        libgcc-s1 \
        libc6 \
        zlib1g \
        liblzma5 \
        ca-certificates

    # Install Python packages that need MPI
    # mpi4py will use the copied MPI libraries from build stage
    pip install --break-system-packages --no-cache-dir mpi4py NuMPI

    # ================================================================
    # Cleanup to minimize image size
    # ================================================================
    apt-get clean
    rm -rf /var/lib/apt/lists/*
    rm -rf /usr/share/doc /usr/share/man /usr/share/info
    rm -rf /var/cache/apt/archives

    # ================================================================
    # Configure library paths
    # ================================================================
    cat > /etc/ld.so.conf.d/mugrid.conf << 'EOF'
/usr/local/lib
/usr/local/lib/openmpi
/usr/lib
EOF

    # Add CUDA paths (needed for mpi4py even in CPU-only builds)
    if [ -d /usr/local/cuda/lib64 ]; then
        echo "/usr/local/cuda/lib64" >> /etc/ld.so.conf.d/mugrid.conf
        echo "/usr/local/cuda/lib64/stubs" >> /etc/ld.so.conf.d/mugrid.conf
    fi
    if [ -d /opt/rocm/lib ]; then
        echo "/opt/rocm/lib" >> /etc/ld.so.conf.d/mugrid.conf
    fi

    ldconfig

%environment
    export PATH=/usr/local/bin:$PATH
    export LD_LIBRARY_PATH=/usr/local/lib:/usr/local/lib/openmpi:$LD_LIBRARY_PATH
    export PYTHONPATH=/usr/local/lib/python3.12/dist-packages:$PYTHONPATH
    export OPAL_PREFIX=/usr/local
    export LC_ALL=C

%runscript
    exec python3 "$@"

%test
    # Verify basic Python functionality (muGrid test done manually due to env issues)
    python3 -c "import numpy; print('NumPy version:', numpy.__version__)"

%help
    muGrid - Minimal Runtime Container
    ==================================

    Multi-stage build for minimal container size.
    Contains only runtime libraries, no compilers or development tools.

    Build info: /etc/mugrid-build-info

    Usage:
      apptainer run mugrid.sif script.py
      apptainer exec mugrid.sif python3 -c "import muGrid"

      # With MPI:
      srun apptainer exec mugrid.sif python3 script.py

      # With GPU:
      srun apptainer exec --nv mugrid.sif ...    # NVIDIA
      srun apptainer exec --rocm mugrid.sif ...  # AMD

%labels
    Author Pastewka Lab
    Version 0.102.0
    Description muGrid minimal runtime (multi-stage build)
