Bootstrap: docker
From: ubuntu:24.04

%help

    This is the base container for running containerized and MPI parallel on
    NEMO2 with ROCm support for MI300A GPUs.

    When running on NEMO2, *do not* load any MPI environment. (Slurm takes
    care of process management and any loaded MPI will collide with the MPI
    installation in the container.) Run the container via

      srun container.sif python_script.py

    in your job submission script.

    To run the MPI benchmark, use

      srun apptainer exec container.sif /opt/mpiBench/mpiBench

    in your job submission script.

    For GPU-aware MPI, ensure you use the UCX transport:

      export OMPI_MCA_pml=ucx

    To compile HIP code with MPI, set:

      export OMPI_CXX=hipcc

%post

    export CONTAINER_HWLOC_VERSION=2.10.0  # like NEMO2
    export CONTAINER_LIBEVENT_VERSION=2.1.12  # like NEMO2
    export CONTAINER_LIBFABRIC_VERSION=1.21.0  # like NEMO2
    export CONTAINER_MUNGE_VERSION=0.5.13  # like NEMO2
    export CONTAINER_SLURM_VERSION=24.05.6 # like NEMO2
    export CONTAINER_UCX_VERSION=1.16.0  # like NEMO2
    export CONTAINER_UCC_VERSION=1.3.0  # like NEMO2
    export CONTAINER_PMIX_VERSION=5.0.2  # like NEMO2
    export CONTAINER_PRRTE_VERSION=3.0.5  # like NEMO2
    export CONTAINER_OMPI_VERSION=5.0.7
    export CONTAINER_MPI4PY_VERSION=4.0.3
    export CONTAINER_ROCM_VERSION=6.4

    # Optimization flags for Zen4 CPUs (MI300A has integrated Zen4 cores)
    # Use -mtune=znver4 instead of -march=znver4 for cross-compilation:
    # -mtune optimizes for Zen4 but produces code that runs on any x86_64
    # -march would require Zen4 instructions which fail on non-Zen4 build machines
    export OPT_FLAGS="-O3 -mtune=znver4"
    export CFLAGS="$OPT_FLAGS"
    export CXXFLAGS="$OPT_FLAGS"
    export FCFLAGS="$OPT_FLAGS"

    # Set environment variable to contain /usr/local
    export PATH=/usr/local/bin:$PATH
    export LD_RUN_PATH=/usr/local/lib:$LD_LIBRARY_PATH
    export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH
    export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH
    export MANPATH=/usr/local/share/man:$MANPATH

    # For apt to be noninteractive
    export DEBIAN_FRONTEND=noninteractive
    export DEBCONF_NONINTERACTIVE_SEEN=true

    # Disable appstream metadata downloads - not needed for container builds
    # and can cause failures during mirror sync
    rm -f /etc/apt/apt.conf.d/50appstream

    # Note: gfortran is necessary even if Fortran is not required, because
    # PnetCDF does not compute compile if OpenMPI does not have Fortran
    # support.
    apt-get update
    apt-get install -y software-properties-common strace libnuma-dev libibverbs-dev libssl-dev curl wget git bash make file pkg-config gcc g++ gfortran python3-dev python3-pip autotools-dev autoconf libtool-bin gnupg ca-certificates

    # Install ROCm using amdgpu-install script
    # This sets up both the AMDGPU and ROCm repositories correctly
    # We only need the userspace libraries, not the kernel driver (DKMS)
    # The kernel driver must be installed on the host system
    wget https://repo.radeon.com/amdgpu-install/${CONTAINER_ROCM_VERSION}/ubuntu/noble/amdgpu-install_${CONTAINER_ROCM_VERSION}.60400-1_all.deb
    apt-get install -y ./amdgpu-install_${CONTAINER_ROCM_VERSION}.60400-1_all.deb
    rm amdgpu-install_${CONTAINER_ROCM_VERSION}.60400-1_all.deb
    # Remove appstream config again in case amdgpu-install restored it
    rm -f /etc/apt/apt.conf.d/50appstream
    apt-get update
    # Use --no-dkms since we're in a container (kernel driver is on host)
    amdgpu-install -y --usecase=rocm,hiplibsdk --no-dkms

    # Set ROCm environment for subsequent build steps
    export ROCM_PATH=/opt/rocm
    export PATH=$ROCM_PATH/bin:$PATH
    export LD_LIBRARY_PATH=$ROCM_PATH/lib:$LD_LIBRARY_PATH

    # Download, compile and install HWLOC with ROCm support
    rm -rf /tmp/hwloc-${CONTAINER_HWLOC_VERSION}
    curl -L https://download.open-mpi.org/release/hwloc/v2.10/hwloc-${CONTAINER_HWLOC_VERSION}.tar.bz2 | tar -xjC /tmp --no-same-owner
    cd /tmp/hwloc-${CONTAINER_HWLOC_VERSION}
    ./configure --prefix=/usr/local --enable-rocm --with-rocm=$ROCM_PATH
    make -j 4
    make install

    # Download, compile and install LIBFABRIC with ROCm support
    rm -rf /tmp/libfabric-${CONTAINER_LIBFABRIC_VERSION}
    curl -L https://github.com/ofiwg/libfabric/releases/download/v${CONTAINER_LIBFABRIC_VERSION}/libfabric-${CONTAINER_LIBFABRIC_VERSION}.tar.bz2 | tar -xjC /tmp --no-same-owner
    cd /tmp/libfabric-${CONTAINER_LIBFABRIC_VERSION}
    ./configure --prefix=/usr/local --with-rocr=$ROCM_PATH
    make -j 4
    make install

    # Download, compile and install MUNGE
    rm -rf /tmp/munge-${CONTAINER_MUNGE_VERSION}
    curl -L https://github.com/dun/munge/releases/download/munge-${CONTAINER_MUNGE_VERSION}/munge-${CONTAINER_MUNGE_VERSION}.tar.xz | tar -xJC /tmp --no-same-owner
    cd /tmp/munge-${CONTAINER_MUNGE_VERSION}
    ./configure --prefix=/usr/local
    make -j 4
    make install

    # Download, compile and install LIBEVENT
    rm -rf /tmp/libevent-${CONTAINER_LIBEVENT_VERSION}-stable
    curl -L https://github.com/libevent/libevent/releases/download/release-${CONTAINER_LIBEVENT_VERSION}-stable/libevent-${CONTAINER_LIBEVENT_VERSION}-stable.tar.gz | tar -xzC /tmp --no-same-owner
    cd /tmp/libevent-${CONTAINER_LIBEVENT_VERSION}-stable
    ./configure --prefix=/usr/local
    make -j 4
    make install

    # Download, compile and install UCX with ROCm support
    # Check compile options of native UCX on NEMO2 with `ucx_info -b`.
    rm -rf /tmp/ucx-${CONTAINER_UCX_VERSION}
    curl -L https://github.com/openucx/ucx/releases/download/v${CONTAINER_UCX_VERSION}/ucx-${CONTAINER_UCX_VERSION}.tar.gz | tar -xzC /tmp --no-same-owner
    cd /tmp/ucx-${CONTAINER_UCX_VERSION}
    ./configure --prefix=/usr/local --enable-optimizations --enable-cma --enable-mt --with-verbs --with-rocm=$ROCM_PATH --without-java --without-go --disable-doxygen-doc --disable-logging --disable-debug --disable-assertions --disable-params-check
    make -j 4
    make install

    # Download, compile and install UCC with ROCm support
    # Check compile options of native UCC on NEMO2 with `ucc_info -b`.
    # Create a mock amdgpu-arch that returns gfx942 (MI300A) since there's no GPU
    # during container build. The real amdgpu-arch fails with --offload-arch=native.
    mv $ROCM_PATH/lib/llvm/bin/amdgpu-arch $ROCM_PATH/lib/llvm/bin/amdgpu-arch.real
    echo '#!/bin/sh' > $ROCM_PATH/lib/llvm/bin/amdgpu-arch
    echo 'echo gfx942' >> $ROCM_PATH/lib/llvm/bin/amdgpu-arch
    chmod +x $ROCM_PATH/lib/llvm/bin/amdgpu-arch
    rm -rf /tmp/ucc-${CONTAINER_UCC_VERSION}
    curl -L https://github.com/openucx/ucc/archive/refs/tags/v${CONTAINER_UCC_VERSION}.tar.gz | tar -xzC /tmp --no-same-owner
    cd /tmp/ucc-${CONTAINER_UCC_VERSION}
    ./autogen.sh
    ./configure --prefix=/usr/local --with-rocm=$ROCM_PATH --with-ucx=/usr/local
    make -j 4
    make install
    # Restore real amdgpu-arch
    mv $ROCM_PATH/lib/llvm/bin/amdgpu-arch.real $ROCM_PATH/lib/llvm/bin/amdgpu-arch

    # Download, compile and install PMIx
    rm -rf /tmp/pmix-${CONTAINER_PMIX_VERSION}
    curl -L https://github.com/openpmix/openpmix/releases/download/v${CONTAINER_PMIX_VERSION}/pmix-${CONTAINER_PMIX_VERSION}.tar.bz2 | tar -xjC /tmp --no-same-owner
    cd /tmp/pmix-${CONTAINER_PMIX_VERSION}
    ./configure --prefix=/usr/local
    make -j 4
    make install

    # Download, compile and install PRRTE
    rm -rf /tmp/prrte-${CONTAINER_PRRTE_VERSION}
    curl -L https://github.com/openpmix/prrte/releases/download/v${CONTAINER_PRRTE_VERSION}/prrte-${CONTAINER_PRRTE_VERSION}.tar.bz2 | tar -xjC /tmp --no-same-owner
    cd /tmp/prrte-${CONTAINER_PRRTE_VERSION}
    ./configure --prefix=/usr/local
    make -j 4
    make install

    # Download, compile and install OpenMPI with ROCm support
    # Slurm handles starting of processes and initial communication with the
    # process runs through PMIx. We also disable verbs because this
    # leads to warnings that it is not being used anyway.
    # Check compile options of native OpenMPI on JUWELS with `ompi_info`.
    rm -rf /tmp/openmpi-$CONTAINER_OMPI_VERSION
    curl -L https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-${CONTAINER_OMPI_VERSION}.tar.bz2 | tar -xjC /tmp --no-same-owner
    cd /tmp/openmpi-$CONTAINER_OMPI_VERSION
    ./configure --prefix=/usr/local --enable-shared --without-orte --disable-oshmem --without-verbs --without-psm2 --with-hwloc=/usr/local --with-libevent=/usr/local --with-ofi=/usr/local --with-ucx=/usr/local --with-ucc=/usr/local --with-pmix=/usr/local --with-prrte=/usr/local --with-rocm=$ROCM_PATH
    make -j 4
    make install

    # Register shared libraries
    ldconfig

    # Install mpi4py
    python3 -m pip install --break-system-packages --no-binary mpi4py mpi4py==${CONTAINER_MPI4PY_VERSION}

    # Clone and compile MPI benchmark
    cd /opt
    git clone https://github.com/LLNL/mpiBench.git
    cd mpiBench
    make

    # Fetch and compile MPI I/O benchmark
    curl -L https://fs.hlrs.de/projects/par/mpi/b_eff_io/b_eff_io_v2.1.tar.gz | tar -xzC /opt
    cd /opt/b_eff_io
    mpicc -o b_eff_io b_eff_io.c -lm

    # Cleanup to reduce image size
    # Note: Don't use rm -rf /tmp/* as it may include host files during build
    rm -rf /tmp/hwloc-* /tmp/libfabric-* /tmp/munge-* /tmp/libevent-* \
           /tmp/ucx-* /tmp/ucc-* /tmp/pmix-* /tmp/prrte-* /tmp/openmpi-* \
           /tmp/pip-*
    apt-get clean
    rm -rf /var/lib/apt/lists/*

%environment

    # Send output directly to screen
    export PYTHONUNBUFFERED=1
    # Don't load module from $HOME/.local (which is not in the container)
    export PYTHONUSERSITE=1

    export PATH=/usr/local/bin:$PATH
    export LD_RUN_PATH=/usr/local/lib:$LD_LIBRARY_PATH
    export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH
    export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH
    export MANPATH=/usr/local/share/man:$MANPATH

    # ROCm environment
    export ROCM_PATH=/opt/rocm
    export PATH=$ROCM_PATH/bin:$PATH
    export LD_LIBRARY_PATH=$ROCM_PATH/lib:$LD_LIBRARY_PATH

    # GPU target for MI300A
    export HIP_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
    export ROCR_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

%runscript

    python3 "$@"
